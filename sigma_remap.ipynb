{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform sigma coordinate remapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot write to data cache '/glade/p/cesmdata/cseg'. Will not be able to download remote data files. Use environment variable 'CESMDATAROOT' to specify another directory.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import tempfile\n",
    "from subprocess import PIPE, Popen, check_call\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import dask\n",
    "\n",
    "import pop_tools\n",
    "\n",
    "import calc_z_to_sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions\n",
    "\n",
    "Define a set of helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_time_chunks(start, stop, chunk_size):\n",
    "    \"\"\"\n",
    "    Generate a list of index pairs for brute-force chunking in time.\n",
    "    \"\"\"    \n",
    "    time_level_count = stop - start\n",
    "    nchunk =  time_level_count // chunk_size\n",
    "\n",
    "    if time_level_count%chunk_size != 0:\n",
    "        nchunk += 1\n",
    "    \n",
    "    return [\n",
    "        (start +i * chunk_size, start + i * chunk_size + chunk_size)\n",
    "        for i in range(nchunk -1)] + [(start + (nchunk - 1 ) * chunk_size, stop)]\n",
    "\n",
    "\n",
    "def filename_chunk(file_in, time_index, dir_out):\n",
    "    \"\"\"\n",
    "    Create a filename for a chunked file.\n",
    "    \"\"\"\n",
    "    file_in = os.path.basename(file_in).replace('.nc', '')   \n",
    "    return f'{dir_out}/{file_in}.tnx.{time_index[0]:03d}-{time_index[1]:03d}.nc'\n",
    "\n",
    "\n",
    "def write_input_file(file_in, time_index, dir_out, clobber):      \n",
    "    \"\"\"\n",
    "    Read a file and write a time-indexed subset to a new file.\n",
    "    \n",
    "    Parameters\n",
    "    ----------    \n",
    "    file_in : str\n",
    "      The input filename\n",
    "    time_index : tuple\n",
    "       The upper and lower index bounds\n",
    "    dir_out : str\n",
    "       The output directory.\n",
    "    clobber : boolean\n",
    "       Overwrite all files.\n",
    "       \n",
    "    Returns\n",
    "    -------\n",
    "    file_out : str\n",
    "      The output file that was written.\n",
    "      \n",
    "    \"\"\"\n",
    "    file_out = filename_chunk(file_in, time_index, dir_out) \n",
    "    \n",
    "    if os.path.exists(file_out) and clobber:\n",
    "        check_call(['rm', '-f', file_out])\n",
    "               \n",
    "    if not os.path.exists(file_out):         \n",
    "        os.makedirs(os.path.dirname(file_out), exist_ok=True)\n",
    "        \n",
    "        time_slice = slice(time_index[0], time_index[1])\n",
    "\n",
    "        open_kwargs = dict(decode_coords=False, decode_times=False, decode_cf=False) \n",
    "        with xr.open_dataset(file_in, **open_kwargs) as ds:\n",
    "            ds.isel(time=time_slice).to_netcdf(file_out)\n",
    "\n",
    "    return file_out \n",
    "\n",
    "@dask.delayed\n",
    "def ncrcat(files, file_out):\n",
    "    \"\"\"\n",
    "    Call ncrcat\n",
    "    \n",
    "    Parameter\n",
    "    ---------\n",
    "    files : list\n",
    "      The files to concatentate\n",
    "    file_out : str\n",
    "      The output file.      \n",
    "    \"\"\"\n",
    "    \n",
    "    tmpfile = tempfile.NamedTemporaryFile(suffix='.filelist')\n",
    "    with open(tmpfile.name, 'w') as fid:\n",
    "        for f in files:\n",
    "            fid.write(f'{f}\\n')\n",
    "\n",
    "    ncrcat_cmd = ' '.join(['cat', tmpfile.name, '|', 'ncrcat', '-O', '-o', file_out])\n",
    "    cmd = ' && '.join(['module load nco', ncrcat_cmd])\n",
    "    \n",
    "    p = Popen(cmd, stdout=PIPE, stderr=PIPE, shell=True)\n",
    "    \n",
    "    stdout, stderr = p.communicate()\n",
    "    if p.returncode != 0:\n",
    "        print(stdout.decode('UTF-8'))\n",
    "        print(stderr.decode('UTF-8'))\n",
    "        raise   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spin up dask cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1af1c787b2f34418b2b5666b5a5b4e3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>NCARCluster</h2>'), HBox(children=(HTML(value='\\n<div>\\n  <style scoped>\\n    .â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ncar_jobqueue import NCARCluster\n",
    "cluster = NCARCluster()\n",
    "cluster.scale(36)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://128.117.181.225:45870</li>\n",
       "  <li><b>Dashboard: </b><a href='https://jupyterhub.ucar.edu/dav/user/mclong/proxy/8787/status' target='_blank'>https://jupyterhub.ucar.edu/dav/user/mclong/proxy/8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>0</li>\n",
       "  <li><b>Cores: </b>0</li>\n",
       "  <li><b>Memory: </b>0 B</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://128.117.181.225:45870' processes=0 threads=0, memory=0 B>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dask.distributed import Client\n",
    "client = Client(cluster) # Connect this local process to remote workers\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup problem details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "droot = '/glade/scratch/kristenk/archive/cesm22cocco.G1850ECOIAF.T62_g17.003/ocn/proc/tseries/month_1'\n",
    "\n",
    "case = 'cesm22cocco.G1850ECOIAF.T62_g17.003'\n",
    "stream = 'pop.h'\n",
    "datestr = '024901-031012'\n",
    "variables = ['ALK', 'ALK_ALT_CO2', 'DIC',]\n",
    "\n",
    "\n",
    "USER = os.environ['USER']\n",
    "dout = f'/glade/scratch/{USER}/besome/cesm_cases/{case}/sigma_coord'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the list of input files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = {}\n",
    "for v in variables+['PD']:\n",
    "    files[v] = f'{droot}/{case}.{stream}.{v}.{datestr}.nc'\n",
    "    if not os.path.exists(files[v]):\n",
    "        raise ValueError(f'{files[v]} dne')\n",
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the sigma coordinate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_delta=0.2\n",
    "sigma_start=24. - sigma_delta / 2\n",
    "sigma_stop=28. +  sigma_delta / 2\n",
    "\n",
    "sigma_edges = calc_z_to_sigma.sigma_coord_edges(sigma_start,sigma_stop,sigma_delta)\n",
    "sigma = np.average(np.vstack((sigma_edges[0:-1],sigma_edges[1:])),axis=0)\n",
    "sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the chunks over-which to apply brute-force parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with xr.open_dataset(files['PD'], decode_coords=False, decode_times=False) as ds:\n",
    "    stop = len(ds.time)\n",
    "\n",
    "time_chunks = gen_time_chunks(start=0, stop=stop, chunk_size=12)\n",
    "print(f'{len(time_chunks)} chunks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate sigma2 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open_kwargs = dict(decode_coords=False, decode_times=False, decode_cf=False) \n",
    "\n",
    "#dsS =  xr.open_dataset(files['SALT'], chunks={'time': 12}, **open_kwargs)\n",
    "#dsT =  xr.open_dataset(files['TEMP'], chunks={'time': 12}, **open_kwargs)\n",
    "\n",
    "#sigma2 = pop_tools.eos(salt=dsS.SALT, temp=dsT.TEMP, pressure=2000.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform the remapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute ALK\n",
      "\tremapping time chunks...\n",
      "compute ALK_ALT_CO2\n",
      "\tremapping time chunks...\n",
      "compute DIC\n",
      "\tremapping time chunks...\n",
      "concatenating chunks...\n",
      "CPU times: user 2min 37s, sys: 18.9 s, total: 2min 56s\n",
      "Wall time: 1h 11min 31s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None, None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "@dask.delayed\n",
    "def op_one_chunk(file_in_var, file_in_sigma, file_out, time_index, dir_out, clobber):\n",
    "    \"\"\"\n",
    "    Remap one chunk to sigma coord.\n",
    "    \"\"\"\n",
    "    \n",
    "    dir_tmp = f'{dir_out}/tmp'\n",
    "               \n",
    "    # write the input files\n",
    "    file_in_var_sub = write_input_file(file_in_var, time_index, dir_tmp, clobber)\n",
    "    file_in_sigma_sub = write_input_file(file_in_sigma, time_index, dir_tmp, clobber)\n",
    "     \n",
    "    # construct output file name\n",
    "    file_out = filename_chunk(file_out, time_index, dir_tmp)\n",
    "    \n",
    "    if os.path.exists(file_out) and clobber:\n",
    "        check_call(['rm', '-f', file_out])\n",
    "    \n",
    "    # call the remapping routine\n",
    "    if not os.path.exists(file_out):\n",
    "        os.makedirs(os.path.dirname(file_out), exist_ok=True)\n",
    "        calc_z_to_sigma.compute(\n",
    "            file_in=file_in_var_sub,\n",
    "            file_in_sigma=file_in_sigma_sub, \n",
    "            file_out=file_out,\n",
    "            zname='z_t', \n",
    "            dzname='dz', \n",
    "            kmtname='KMT', \n",
    "            sigma_varname='PD',\n",
    "            convert_from_pd=True,\n",
    "            dimsub={},\n",
    "            sigma_start=sigma_start,\n",
    "            sigma_stop=sigma_stop,\n",
    "            sigma_delta=sigma_delta,)    \n",
    "        \n",
    "    return file_out\n",
    "\n",
    "\n",
    "def op_var(file_in_var, file_in_sigma, file_out, dir_out, time_chunks, clobber):\n",
    "    \"\"\"\n",
    "    Remap variable to sigma coords, chunking in time.\n",
    "    \"\"\"\n",
    "    kw=dict(\n",
    "        file_in_var=file_in_var, \n",
    "        file_in_sigma=file_in_sigma, \n",
    "        file_out=file_out,\n",
    "        dir_out=dir_out,\n",
    "        clobber=clobber,\n",
    "    )\n",
    "    \n",
    "    file_cat = [op_one_chunk(time_index=tnx, **kw) for tnx in time_chunks]    \n",
    "    \n",
    "    return dask.compute(*file_cat)\n",
    "\n",
    "\n",
    "# loop over variables and remap each one\n",
    "clobber = False\n",
    "ncrcat_delayed = []\n",
    "for v in variables:\n",
    "    \n",
    "    file_out = f'{dout}/{case}.{stream}.sigma.{v}.{datestr}.nc'    \n",
    "\n",
    "    if not os.path.exists(file_out) or clobber:         \n",
    "        os.makedirs(os.path.dirname(file_out), exist_ok=True)\n",
    "\n",
    "        print(f'compute {v} remapping')\n",
    "        file_cat = op_var(files[v], files['PD'], file_out, dout, time_chunks, clobber)\n",
    "        \n",
    "        ncrcat_delayed.append(ncrcat(file_cat, file_out))\n",
    "\n",
    "print('concatenating chunks...')\n",
    "dask.compute(*ncrcat_delayed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up temporary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_call(['rm', '-frv', f'{dout}/tmp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3-besome]",
   "language": "python",
   "name": "conda-env-miniconda3-besome-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
